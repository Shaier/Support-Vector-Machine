{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf we have linearly separable data in two dimensions, a simple SVM will try to find a boundary that divides the data in \\na way that the misclassification error can be minimized.\\nBasically, SVM finds the most optimal decision boundary (the one which has maximum margin from the nearest points of all the classes).\\nThe nearest points from the decision boundary that maximize the distance between the decision boundary and the points are called support vectors\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Wiki:\n",
    "Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model\n",
    "that assigns new examples to one category or the other.\n",
    "examples of the separate categories are divided by a clear gap that is as wide as possible.\n",
    "New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n",
    "'''\n",
    "\n",
    "#Simple SVM\n",
    "'''\n",
    "If we have linearly separable data in two dimensions, a simple SVM will try to find a boundary that divides the data in \n",
    "a way that the misclassification error can be minimized.\n",
    "Basically, SVM finds the most optimal decision boundary (the one which has maximum margin from the nearest points of all the classes).\n",
    "The nearest points from the decision boundary that maximize the distance between the decision boundary and the points are called support vectors\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data- bill_authentication dataset\n",
    "path='C:\\\\Users\\\\sagi\\\\bill_authentication.csv'\n",
    "bankdata=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1372, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variance</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Curtosis</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variance  Skewness  Curtosis  Entropy  Class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explore the data\n",
    "print(bankdata.shape)\n",
    "bankdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the data\n",
    "#(1) Dividing the data into attributes and labels\n",
    "#(2) dividing the data into training and testing sets.\n",
    "\n",
    "#attributes and labels\n",
    "#x is the attributes (what we're using to predict)\n",
    "x=bankdata.drop('Class', axis=1) #store all the columns of the bankdata except for class col.\n",
    "#y is the label (what we're trying to predict)\n",
    "y=bankdata['Class'] #store all the class col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "#Training the Algorithm\n",
    "'''The svm library contains built-in classes for different SVM algorithms.\n",
    "Since we are going to perform a classification task, we will use the support vector classifier class, which is written as SVC.\n",
    "It takes one parameter, which is the kernel type. This is very important. In the case of a simple SVM we simply set this \n",
    "parameter as \"linear\" since simple SVMs can ONLY classify linearly separable data.\n",
    "'''\n",
    "#use the fit method to train the algorithm on the training data which is passed as a parameter to the fit method\n",
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Predictions\n",
    "y_predict=svclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144   2]\n",
      " [  1 128]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       146\n",
      "           1       0.98      0.99      0.99       129\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       275\n",
      "   macro avg       0.99      0.99      0.99       275\n",
      "weighted avg       0.99      0.99      0.99       275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the Algorithm\n",
    "#Confusion matrix, precision, recall, and F1 measures are the most commonly used metrics for classification tasks\n",
    "#Confusion matrix: summarizing the performance of a classification algorithm- The number of correct and incorrect predictions are shown and broken summarized by each class,\n",
    "#precision: True positive/ (T positive+ F positive)\n",
    "#Recall: T pos/ (T pos + F neg)\n",
    "#F1:a function of Precision and Recall : --> 2* (precision*Recall)/(prec.+ rec)\n",
    "#Need when you need a balance between rec and prec.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_predict))\n",
    "print(classification_report(y_test, y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
